{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from io import open\n",
    "import torch\n",
    "from dataloaders.datasets.voc_dataset import VOCDataset\n",
    "import torch.quantization\n",
    "import torch.quantization\n",
    "from utils import helpers\n",
    "from Project import project\n",
    "\n",
    "from models.utils import (\n",
    "    create_mobilenetv2_ssd_lite,\n",
    "    create_mobilenetv2_ssd_lite_predictor,\n",
    ")\n",
    "from torch.quantization import (\n",
    "    MovingAveragePerChannelMinMaxObserver,\n",
    "    PerChannelMinMaxObserver,\n",
    "    QConfig,\n",
    "    default_per_channel_weight_observer,\n",
    ")\n",
    "from models import mobilenet_v2\n",
    "from evaluation import model_evaluation\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelQuantizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def quantize_def(self, model):\n",
    "        model.eval().to(\"cpu\")\n",
    "        model.fuse_model()\n",
    "        model.qconfig = torch.quantization.default_per_channel_qconfig\n",
    "        torch.quantization.prepare(model, inplace=True)\n",
    "        torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "    def quantize(self, model: mobilenet_v2.MobileNetV2):\n",
    "        model.eval().to(\"cpu\")\n",
    "\n",
    "        model.fuse_model()\n",
    "        # default_per_channel_weight_observer\n",
    "\n",
    "        model.qconfig = torch.quantization.default_per_channel_qconfig\n",
    "        #     QConfig(\n",
    "        #     activation=PerChannelMinMaxObserver.with_args(dtype=torch.qint8),\n",
    "        #     weight=PerChannelMinMaxObserver.with_args(\n",
    "        #         dtype=torch.qint8,\n",
    "        #         qscheme=torch.per_channel_symmetric\n",
    "        #     )\n",
    "        # )\n",
    "        # model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "\n",
    "        torch.quantization.prepare(model, inplace=True)\n",
    "        # We should calibrate here\n",
    "        print(\"Calibrating\")\n",
    "        self._calibrate(model, 10)\n",
    "        print(\"Done Calibrating\")\n",
    "        torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "    def _calibrate(self, model, eval_steps: int = 100):\n",
    "        dataset = VOCDataset(project.train_data_dir, is_test=False)\n",
    "\n",
    "        predictor = create_mobilenetv2_ssd_lite_predictor(\n",
    "            model, nms_method=\"hard\", device=torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "        for i in tqdm(range(len(dataset)), total=eval_steps):\n",
    "            if i == eval_steps:\n",
    "                break\n",
    "            image = dataset.get_image(i)\n",
    "            boxes, labels, probs = predictor.predict(image)\n",
    "            indexes = torch.ones(labels.size(0), 1, dtype=torch.float32) * i\n",
    "\n",
    "    def uninplace(self, model):\n",
    "        \"\"\"Sets all `inplace` values to False\"\"\"\n",
    "        if hasattr(model, \"inplace\"):\n",
    "            model.inplace = False\n",
    "        if not model.children():\n",
    "            return\n",
    "        for child in model.children():\n",
    "            self.uninplace(child)\n",
    "\n",
    "    def _prep_for_fusion(self, model, parent_name):\n",
    "        \"\"\"Fuses all conv+bn+relu, conv+bn, and conv+relu\"\"\"\n",
    "        if not model.children():\n",
    "            return []\n",
    "        result = []\n",
    "        candidate = []\n",
    "        for name, child in model.named_children():\n",
    "            new_name = parent_name + \".\" + name\n",
    "            if new_name[0] == \".\":\n",
    "                new_name = new_name[1:]\n",
    "            if type(child) == torch.nn.Sequential:\n",
    "                candidate = []\n",
    "                result.extend(self._prep_for_fusion(child, new_name))\n",
    "            else:\n",
    "                if len(candidate) == 0 and type(child) == torch.nn.Conv2d:\n",
    "                    candidate = [new_name]\n",
    "                elif len(candidate) == 1 and type(child) == torch.nn.ReLU:\n",
    "                    candidate.append(new_name)\n",
    "                    result.append(candidate)\n",
    "                    candidate = []\n",
    "                elif len(candidate) == 1 and type(child) == torch.nn.BatchNorm2d:\n",
    "                    candidate.append(new_name)\n",
    "                elif len(candidate) == 2:\n",
    "                    if type(child) == torch.nn.ReLU:\n",
    "                        candidate.append(new_name)\n",
    "                    result.append(candidate)\n",
    "                    candidate = []\n",
    "        return result\n",
    "\n",
    "    def scaled_quantization(self, model, scale=1e-3, zero_point=128):\n",
    "        model.eval()\n",
    "        self.uninplace(model)\n",
    "        modules_to_fuse = self._prep_for_fusion(model, \"\")\n",
    "        model.qconfig = torch.quantization.default_qconfig\n",
    "        fused_model = torch.quantization.fuse_modules(\n",
    "            model, modules_to_fuse, inplace=True\n",
    "        )\n",
    "\n",
    "        fused_model = torch.quantization.prepare(fused_model, inplace=True)\n",
    "        torch.quantization.convert(fused_model, inplace=True)\n",
    "\n",
    "    def save_model(self, model):\n",
    "        file_name = helpers.get_time_of_day() + \".pth\"\n",
    "        torch.save(model.state_dict(), project.quantized_trained_model_dir / file_name)\n",
    "        return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SSD' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2b34a2c83b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#---------------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_concrete_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/navya/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36mfrom_concrete_functions\u001b[0;34m(cls, funcs)\u001b[0m\n\u001b[1;32m    332\u001b[0m       \u001b[0mInvalid\u001b[0m \u001b[0minput\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \"\"\"\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcreteFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This function takes in a list of ConcreteFunction.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'SSD' object is not iterable"
     ]
    }
   ],
   "source": [
    "model_path = 'trained_models/new-def-trained/mb2-ssd-lite-Epoch-119-Loss-4.837355399743105.pth'\n",
    "label_path = project.trained_model_dir / \"voc-model-labels.txt\"\n",
    "\n",
    "class_names = [name.strip() for name in open(label_path).readlines()]\n",
    "\n",
    "net = create_mobilenetv2_ssd_lite(len(class_names), is_test=True)\n",
    "net.load(model_path)\n",
    "\n",
    "#Tensorflow conversion\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(net)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"/testing\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file = tflite_models_dir/\"voc_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "\n",
    "#---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing the model\n",
      "Calibrating\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19667316e864f2db42cfd88a533b19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Calibrating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/anaconda3/envs/navya/lib/python3.7/site-packages/torch/quantization/observer.py:131: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  Returning default scale and zero point \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the quantized model\n",
      "\n",
      "Quantinzed the vanilla model to get the correct definition\n",
      "Calibrating\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64593818d30f4bb68ef51c9de87ecf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Calibrating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5011 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the quantized weights\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 276/5011 [00:24<09:11,  8.59it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-753c6b089d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mroot_eval_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_results_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mstopping_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m )\n",
      "\u001b[0;32m~/Navya/quantization/quantized-object-detector-demo/evaluation/model_evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, predictor, dataset, class_names, root_eval_dir, iou_threshold, is_pruned, pruning_strategy, pruning_percentage, stopping_point)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         results, mean_inference_time, std_inference_time = self._evaluate_dataset(\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopping_point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Navya/quantization/quantized-object-detector-demo/evaluation/model_evaluation.py\u001b[0m in \u001b[0;36m_evaluate_dataset\u001b[0;34m(self, predictor, dataset, stopping_point)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0minference_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Navya/quantization/quantized-object-detector-demo/models/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, image, top_k, prob_threshold)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mcandidate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             )\n\u001b[1;32m     80\u001b[0m             \u001b[0mpicked_box_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Navya/quantization/quantized-object-detector-demo/vision/utils/box_utils.py\u001b[0m in \u001b[0;36mnms\u001b[0;34m(box_scores, nms_method, score_threshold, iou_threshold, sigma, top_k, candidate_size)\u001b[0m\n\u001b[1;32m    251\u001b[0m                         \u001b[0miou_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                         \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                         candidate_size=candidate_size)\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Navya/quantization/quantized-object-detector-demo/vision/utils/box_utils.py\u001b[0m in \u001b[0;36mhard_nms\u001b[0;34m(box_scores, iou_threshold, top_k, candidate_size)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mcurrent_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         )\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miou\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbox_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpicked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_quantizer = ModelQuantizer()\n",
    "print(\"Quantizing the model\")\n",
    "model_quantizer.quantize(net)\n",
    "model_quantizer.save_model(net)\n",
    "print(\"Saving the quantized model\")\n",
    "file_name = model_quantizer.save_model(net)\n",
    "\n",
    "q_model = create_mobilenetv2_ssd_lite(len(class_names), is_test=True)\n",
    "print(\"Quantinzed the vanilla model to get the correct definition\")\n",
    "model_quantizer.quantize(q_model)\n",
    "print(\"Loading the quantized weights\")\n",
    "q_model.load(project.quantized_trained_model_dir / file_name)\n",
    "\n",
    "dataset = VOCDataset(project.train_data_dir, is_test=False)\n",
    "\n",
    "predictor = create_mobilenetv2_ssd_lite_predictor(\n",
    "    net, nms_method=\"hard\", device=torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "model_evaluator = model_evaluation.ModelEvaluator(\n",
    "    description=\"Quantized model that is trained from Navya\"\n",
    ")\n",
    "\n",
    "while len(tqdm._instances) > 0:\n",
    "    tqdm._instances.pop().close()\n",
    "\n",
    "model_evaluator.evaluate(\n",
    "    predictor=predictor,\n",
    "    dataset=dataset,\n",
    "    class_names=class_names,\n",
    "    root_eval_dir=project.eval_results_dir,\n",
    "    stopping_point=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
